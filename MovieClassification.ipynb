{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('MovieClassification_nlp_ML').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read dataset\n",
    "textRevs_df=spark.read.csv('C:/Users/Owner/Documents/Machine Learning/Movie_reviews.csv', inferSchema=True,header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#know the columns and their datatypes\n",
    "textRevs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7087"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the number of rows\n",
    "textRevs_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+---------+\n",
      "|Review                                                                  |Sentiment|\n",
      "+------------------------------------------------------------------------+---------+\n",
      "|Because I would like to make friends who like the same things I like, an|1        |\n",
      "|Mission Impossible 3 was excellent.                                     |1        |\n",
      "|As I sit here, watching the MTV Movie Awards, I am reminded of how much |0        |\n",
      "|I am going to start reading the Harry Potter series again because that i|1        |\n",
      "|I love Harry Potter.                                                    |1        |\n",
      "|da vinci code was an awesome movie...                                   |1        |\n",
      "|I love Harry Potter.                                                    |1        |\n",
      "|I love Harry Potter..                                                   |1        |\n",
      "|Is it just me, or does Harry Potter suck?...                            |0        |\n",
      "|I love Harry Potter.                                                    |1        |\n",
      "+------------------------------------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select random rows from the dataset to see how they're.. using rand function\n",
    "from pyspark.sql.functions import rand \n",
    "textRevs_df.orderBy(rand()).show(10,False)  # Note Sentiment values are read in as string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because they are of string data type, they are left aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6990"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As sentiment column has values other than 0 and 1, we are going to filter and make a fresh dataset using filter \n",
    "textRevs_df = textRevs_df.filter(((textRevs_df.Sentiment =='1') | (textRevs_df.Sentiment =='0')))\n",
    "textRevs_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|Sentiment|count|\n",
      "+---------+-----+\n",
      "|        0| 3081|\n",
      "|        1| 3909|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Checking distribution of rows for each sentiment values of 0 and 1\n",
    "textRevs_df.groupBy('Sentiment').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check the columns and their datatypes using printSchema\n",
    "textRevs_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Sentiment value read in is not a float, required by LR. So, we have to convert it. \n",
    " Since we will end up with duplicate columns with the same data, we will drop the original\n",
    " sentiment column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If model has more positive reviews than negative reviews, ml will lean towards positive.. so the closer the groups of divisions of 0 and 1 are, the more accurate the model will be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRevs_df = textRevs_df.withColumn(\"Label\", textRevs_df.Sentiment.cast('float')).drop('Sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above thing can be done with stringindexer too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textRevs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------+-----+\n",
      "|Review                                                      |Label|\n",
      "+------------------------------------------------------------+-----+\n",
      "|Brokeback Mountain was boring.                              |0.0  |\n",
      "|mission impossible 2 rocks!!....                            |1.0  |\n",
      "|Brokeback Mountain was so awesome.                          |1.0  |\n",
      "|Is it just me, or does Harry Potter suck?...                |0.0  |\n",
      "|Love luv lubb the Da Vinci Code!                            |1.0  |\n",
      "|He's like,'YEAH I GOT ACNE AND I LOVE BROKEBACK MOUNTAIN '..|1.0  |\n",
      "|Brokeback Mountain was boring.                              |0.0  |\n",
      "|Brokeback Mountain was boring.                              |0.0  |\n",
      "|mission impossible 2 rocks!!....                            |1.0  |\n",
      "|mission impossible 2 rocks!!....                            |1.0  |\n",
      "+------------------------------------------------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textRevs_df.orderBy(rand()).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is not clean.. there are many punctuations which are unneccessary. When we tokenize its gonna convert it to a word. Sometimes lengths of sentences also differ.\n",
    "We're gonna look into these kinds of things before we analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again looking at data to make sure we haven't lost any during trnasformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 3909|\n",
      "|  0.0| 3081|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textRevs_df.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add length to the dataframe\n",
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRevs_df=textRevs_df.withColumn('length',length(textRevs_df['Review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+\n",
      "|              Review|Label|length|\n",
      "+--------------------+-----+------+\n",
      "|Always knows what...|  0.0|    61|\n",
      "|So as felicia's m...|  1.0|    71|\n",
      "|I LOVE THE DA VIN...|  1.0|    28|\n",
      "|I think I hate Ha...|  0.0|    72|\n",
      "|The Da Vinci Code...|  1.0|    30|\n",
      "+--------------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textRevs_df.orderBy(rand()).show(5,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop word remover removes vocab but punctuations wont be removed.\n",
    "agg is performed on length and finds avg of set of values in that column\n",
    "\n",
    "\n",
    "To handle punctuations, create a function having a list of punctuations and check for all the reviews with the function and replace them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Label|      avg(Length)|\n",
      "+-----+-----------------+\n",
      "|  1.0|47.61882834484523|\n",
      "|  0.0|50.95845504706264|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textRevs_df.groupBy('Label').agg({'Length':'mean'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can observe that negative comments are 3 more than positive comments. This is nothing to be bothered about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, trim, col, lower\n",
    "\n",
    "def removePunctuation(column):\n",
    "    return trim(lower(regexp_replace(column, '([^\\s\\w_]|_)+', '')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "textRevs_df = textRevs_df.withColumn('review_nopunct', removePunctuation(col('Review')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review_nopunct column has all the punctuations removed from the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- review_nopunct: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textRevs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+-----+------+----------------------------------------------------------------------+\n",
      "|Review                                                                  |Label|length|review_nopunct                                                        |\n",
      "+------------------------------------------------------------------------+-----+------+----------------------------------------------------------------------+\n",
      "|The Da Vinci Code book is just awesome.                                 |1.0  |39    |the da vinci code book is just awesome                                |\n",
      "|this was the first clive cussler i've ever read, but even books like Rel|1.0  |72    |this was the first clive cussler ive ever read but even books like rel|\n",
      "|i liked the Da Vinci Code a lot.                                        |1.0  |32    |i liked the da vinci code a lot                                       |\n",
      "|i liked the Da Vinci Code a lot.                                        |1.0  |32    |i liked the da vinci code a lot                                       |\n",
      "|I liked the Da Vinci Code but it ultimatly didn't seem to hold it's own.|1.0  |72    |i liked the da vinci code but it ultimatly didnt seem to hold its own |\n",
      "+------------------------------------------------------------------------+-----+------+----------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textRevs_df.show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation for NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------+\n",
      "|tokens                                                                                |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "|[the, da, vinci, code, book, is, just, awesome]                                       |\n",
      "|[this, was, the, first, clive, cussler, ive, ever, read, but, even, books, like, rel] |\n",
      "|[i, liked, the, da, vinci, code, a, lot]                                              |\n",
      "|[i, liked, the, da, vinci, code, a, lot]                                              |\n",
      "|[i, liked, the, da, vinci, code, but, it, ultimatly, didnt, seem, to, hold, its, own] |\n",
      "|[thats, not, even, an, exaggeration, , and, at, midnight, we, went, to, walmart, to]  |\n",
      "|[i, loved, the, da, vinci, code, but, now, i, want, something, better, and, different]|\n",
      "|[i, thought, da, vinci, code, was, great, same, with, kite, runner]                   |\n",
      "|[the, da, vinci, code, is, actually, a, good, movie]                                  |\n",
      "|[i, thought, the, da, vinci, code, was, a, pretty, good, book]                        |\n",
      "+--------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer splits the sentence into words and also all the letters are converted to lower cases.\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenization=Tokenizer(inputCol='review_nopunct',outputCol='tokens')\n",
    "tokenized_df=tokenization.transform(textRevs_df)\n",
    "tokenized_df.select('tokens').show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n",
      "|refined_tokens                                                  |\n",
      "+----------------------------------------------------------------+\n",
      "|[da, vinci, code, book, awesome]                                |\n",
      "|[first, clive, cussler, ive, ever, read, even, books, like, rel]|\n",
      "|[liked, da, vinci, code, lot]                                   |\n",
      "|[liked, da, vinci, code, lot]                                   |\n",
      "|[liked, da, vinci, code, ultimatly, didnt, seem, hold]          |\n",
      "|[thats, even, exaggeration, , midnight, went, walmart]          |\n",
      "|[loved, da, vinci, code, want, something, better, different]    |\n",
      "|[thought, da, vinci, code, great, kite, runner]                 |\n",
      "|[da, vinci, code, actually, good, movie]                        |\n",
      "|[thought, da, vinci, code, pretty, good, book]                  |\n",
      "+----------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Some very common words such as 'this', 'the', 'to' etc are known as stop words. \n",
    "#In order to decrease the computation overhead, its always a good idea to drop them\n",
    "#hence we use stopwordsremover\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopword_removal=StopWordsRemover(inputCol='tokens',outputCol='refined_tokens')\n",
    "refined_df=stopword_removal.transform(tokenized_df)\n",
    "refined_df.select(['refined_tokens']).show(10,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything's converted to lowercase and then breaks into individual words by tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- review_nopunct: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- refined_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a count of tokens in each movie\n",
    "from pyspark.sql.functions import size\n",
    "refined_df = refined_df.select('*',size('refined_tokens').alias('token_count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "size counts no. of items in a list.. as we can see the review tokens were in the list []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------+-----+------+-------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------+-----------+\n",
      "|Review                                                                |Label|length|review_nopunct                                                     |tokens                                                                             |refined_tokens                                         |token_count|\n",
      "+----------------------------------------------------------------------+-----+------+-------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------+-----------+\n",
      "|I love The Da Vinci Code...                                           |1.0  |27    |i love the da vinci code                                           |[i, love, the, da, vinci, code]                                                    |[love, da, vinci, code]                                |4          |\n",
      "|You know, the Harry Potter books are decent enough, and I ’ m glad the|1.0  |70    |you know the harry potter books are decent enough and i  m glad the|[you, know, the, harry, potter, books, are, decent, enough, and, i, , m, glad, the]|[know, harry, potter, books, decent, enough, , m, glad]|9          |\n",
      "|I love The Da Vinci Code...                                           |1.0  |27    |i love the da vinci code                                           |[i, love, the, da, vinci, code]                                                    |[love, da, vinci, code]                                |4          |\n",
      "|He's like,'YEAH I GOT ACNE AND I LOVE BROKEBACK MOUNTAIN '..          |1.0  |60    |hes likeyeah i got acne and i love brokeback mountain              |[hes, likeyeah, i, got, acne, and, i, love, brokeback, mountain]                   |[hes, likeyeah, got, acne, love, brokeback, mountain]  |7          |\n",
      "+----------------------------------------------------------------------+-----+------+-------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------+-----------+\n",
      "only showing top 4 rows\n",
      "\n",
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- review_nopunct: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- refined_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_count: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# display a count of tokens in each movie\n",
    "refined_df.orderBy(rand()).show(4, False)\n",
    "refined_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+-----------+\n",
      "|refined_tokens                                                  |token_count|\n",
      "+----------------------------------------------------------------+-----------+\n",
      "|[da, vinci, code, book, awesome]                                |5          |\n",
      "|[first, clive, cussler, ive, ever, read, even, books, like, rel]|10         |\n",
      "|[liked, da, vinci, code, lot]                                   |5          |\n",
      "|[liked, da, vinci, code, lot]                                   |5          |\n",
      "|[liked, da, vinci, code, ultimatly, didnt, seem, hold]          |8          |\n",
      "+----------------------------------------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "refined_df.select(['refined_tokens','token_count']).show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "review length and token count give us similar kind of info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes each word in the BoW and counts how many times that word appears in each document. It is basically computing Term Frequency (TF) or the number of times each word occurs in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vec = CountVectorizer(inputCol='refined_tokens', outputCol='cv_features')\n",
    "cv_df = count_vec.fit(refined_df).transform(refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+----------------------------------------------------------------------------------------+\n",
      "|refined_tokens                                                  |cv_features                                                                             |\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------------------------------+\n",
      "|[da, vinci, code, book, awesome]                                |(1706,[0,1,2,8,174],[1.0,1.0,1.0,1.0,1.0])                                              |\n",
      "|[first, clive, cussler, ive, ever, read, even, books, like, rel]|(1706,[13,40,41,185,187,208,214,825,871,1590],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[liked, da, vinci, code, lot]                                   |(1706,[0,1,2,44,189],[1.0,1.0,1.0,1.0,1.0])                                             |\n",
      "|[liked, da, vinci, code, lot]                                   |(1706,[0,1,2,44,189],[1.0,1.0,1.0,1.0,1.0])                                             |\n",
      "|[liked, da, vinci, code, ultimatly, didnt, seem, hold]          |(1706,[0,1,2,44,217,583,890,1140],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                    |\n",
      "|[thats, even, exaggeration, , midnight, went, walmart]          |(1706,[11,38,67,185,1197,1456,1630],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                      |\n",
      "|[loved, da, vinci, code, want, something, better, different]    |(1706,[0,1,2,23,24,75,320,454],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                       |\n",
      "|[thought, da, vinci, code, great, kite, runner]                 |(1706,[0,1,2,55,184,698,818],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                             |\n",
      "|[da, vinci, code, actually, good, movie]                        |(1706,[0,1,2,12,180,203],[1.0,1.0,1.0,1.0,1.0,1.0])                                     |\n",
      "|[thought, da, vinci, code, pretty, good, book]                  |(1706,[0,1,2,174,180,181,184],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])                            |\n",
      "+----------------------------------------------------------------+----------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cv_df.select(['refined_tokens','cv_features']).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['da',\n",
       " 'vinci',\n",
       " 'code',\n",
       " 'harry',\n",
       " 'brokeback',\n",
       " 'potter',\n",
       " 'mountain',\n",
       " 'love',\n",
       " 'awesome',\n",
       " 'mission',\n",
       " 'impossible',\n",
       " '',\n",
       " 'movie',\n",
       " 'like',\n",
       " 'hate',\n",
       " 'sucked',\n",
       " 'sucks',\n",
       " 'much',\n",
       " 'really',\n",
       " 'movies',\n",
       " 'know',\n",
       " 'suck',\n",
       " '3',\n",
       " 'want',\n",
       " 'loved',\n",
       " 'think',\n",
       " 'one',\n",
       " 'stupid',\n",
       " 'depressing',\n",
       " 'horrible',\n",
       " 'reading',\n",
       " 'fucking',\n",
       " 'oh',\n",
       " 'terrible',\n",
       " 'right',\n",
       " 'left',\n",
       " 'ok',\n",
       " 'beautiful',\n",
       " 'went',\n",
       " 'saw',\n",
       " 'read',\n",
       " 'first',\n",
       " '2',\n",
       " 'dont',\n",
       " 'liked',\n",
       " 'absolutely',\n",
       " 'way',\n",
       " 'tom',\n",
       " 'heard',\n",
       " 'big',\n",
       " 'time',\n",
       " 'going',\n",
       " 'film',\n",
       " 'boring',\n",
       " 'said',\n",
       " 'great',\n",
       " 'watch',\n",
       " 'series',\n",
       " 'people',\n",
       " 'man',\n",
       " 'got',\n",
       " 'watching',\n",
       " 'cant',\n",
       " 'b',\n",
       " 'things',\n",
       " 'story',\n",
       " 'last',\n",
       " 'thats',\n",
       " 'friday',\n",
       " 'gay',\n",
       " 'wait',\n",
       " 'person',\n",
       " 'theres',\n",
       " 'cool',\n",
       " 'anyone',\n",
       " 'better',\n",
       " 'excellent',\n",
       " 'says',\n",
       " 'always',\n",
       " 'rocks',\n",
       " 'knows',\n",
       " 'anyway',\n",
       " 'mom',\n",
       " 'friends',\n",
       " 'review',\n",
       " 'worth',\n",
       " 'care',\n",
       " 'opinion',\n",
       " 'never',\n",
       " 'p',\n",
       " 'either',\n",
       " 'stand',\n",
       " 'guy',\n",
       " 'luv',\n",
       " 'needs',\n",
       " 'hat',\n",
       " 'hes',\n",
       " 'make',\n",
       " 'snuck',\n",
       " 'past',\n",
       " 'soo',\n",
       " 'start',\n",
       " 'gonna',\n",
       " 'table',\n",
       " 'head',\n",
       " 'awards',\n",
       " 'hates',\n",
       " 'turned',\n",
       " 'type',\n",
       " 'daniel',\n",
       " 'wanted',\n",
       " 'place',\n",
       " 'crazy',\n",
       " 'whos',\n",
       " 'dads',\n",
       " 'lubb',\n",
       " 'slap',\n",
       " 'leah',\n",
       " 'hella',\n",
       " 'reminded',\n",
       " 'dudeee',\n",
       " '25',\n",
       " 'felicia',\n",
       " 'hung',\n",
       " 'wel',\n",
       " 'kelsie',\n",
       " 'draco',\n",
       " 'deep',\n",
       " 'mat',\n",
       " 'letting',\n",
       " 'sentry',\n",
       " 'likeyeah',\n",
       " 'acne',\n",
       " 'gin',\n",
       " 'wotshisface',\n",
       " 'homosexuality',\n",
       " 'hips',\n",
       " 'escapades',\n",
       " 'grabs',\n",
       " 'malfoy',\n",
       " 'kirsten',\n",
       " 'hill',\n",
       " 'reality',\n",
       " 'laughe',\n",
       " 'bye',\n",
       " 'retarted',\n",
       " 'kate',\n",
       " 'quiz',\n",
       " 'differently',\n",
       " 'combining',\n",
       " 'bonkers',\n",
       " 'desperately',\n",
       " 'sit',\n",
       " 'profound',\n",
       " 'insanely',\n",
       " 'outshines',\n",
       " 'dragged',\n",
       " 'hoot',\n",
       " 'cleaning',\n",
       " 'gary',\n",
       " 'silent',\n",
       " 'coz',\n",
       " 'bobbypin',\n",
       " 'keys',\n",
       " 'stars',\n",
       " 'lovethe',\n",
       " 'zen',\n",
       " 'felicias',\n",
       " 'rig',\n",
       " 'station',\n",
       " 'tha',\n",
       " 'mtv',\n",
       " 'helped',\n",
       " 'trousers',\n",
       " 'book',\n",
       " 'hated',\n",
       " 'also',\n",
       " 'im',\n",
       " 'iii',\n",
       " 'see',\n",
       " 'good',\n",
       " 'pretty',\n",
       " 'though',\n",
       " 'ass',\n",
       " 'thought',\n",
       " 'even',\n",
       " 'say',\n",
       " 'books',\n",
       " 'evil',\n",
       " 'lot',\n",
       " 'well',\n",
       " 'still',\n",
       " 'balls',\n",
       " 'get',\n",
       " 'amazing',\n",
       " 'go',\n",
       " 'kinda',\n",
       " 'already',\n",
       " 'watched',\n",
       " 'miss',\n",
       " 'lol',\n",
       " 'cruise',\n",
       " 'thing',\n",
       " 'actually',\n",
       " 'seen',\n",
       " 'may',\n",
       " 'made',\n",
       " 'yeah',\n",
       " 'ever',\n",
       " 'awful',\n",
       " 'talk',\n",
       " 'tell',\n",
       " 'second',\n",
       " 'id',\n",
       " 'ive',\n",
       " 'best',\n",
       " 'three',\n",
       " 'didnt',\n",
       " 'two',\n",
       " 'us',\n",
       " 'enjoy',\n",
       " 'making',\n",
       " 'enjoyed',\n",
       " 'try',\n",
       " 'new',\n",
       " 'c',\n",
       " 'action',\n",
       " 'ill',\n",
       " 'hey',\n",
       " 'theme',\n",
       " 'm',\n",
       " 'crash',\n",
       " 'sad',\n",
       " 'okay',\n",
       " 'real',\n",
       " 'far',\n",
       " 'night',\n",
       " 'fact',\n",
       " 'totally',\n",
       " 'since',\n",
       " 'looks',\n",
       " 'everyone',\n",
       " 'probably',\n",
       " 'th',\n",
       " 'lord',\n",
       " 'little',\n",
       " 'finished',\n",
       " 'hear',\n",
       " 'won',\n",
       " 'inaccurate',\n",
       " 'course',\n",
       " 'anything',\n",
       " 'school',\n",
       " 'apparently',\n",
       " 'personally',\n",
       " 'song',\n",
       " 'every',\n",
       " 'long',\n",
       " 'youre',\n",
       " 'sick',\n",
       " 'glad',\n",
       " 'sure',\n",
       " 'quite',\n",
       " 'used',\n",
       " 'times',\n",
       " 'demons',\n",
       " 'angels',\n",
       " 'back',\n",
       " 'thinking',\n",
       " 'almost',\n",
       " 'talking',\n",
       " 'mean',\n",
       " 'freaking',\n",
       " 'bogus',\n",
       " 'wrong',\n",
       " 'betterwe',\n",
       " 'saying',\n",
       " 'omg',\n",
       " 'crap',\n",
       " 'tonight',\n",
       " 'luck',\n",
       " 'music',\n",
       " 'stories',\n",
       " 'w',\n",
       " 'show',\n",
       " 'bit',\n",
       " 'move',\n",
       " 'telling',\n",
       " 'officially',\n",
       " 'shitty',\n",
       " 'suc',\n",
       " 'sa',\n",
       " 'shit',\n",
       " 'fan',\n",
       " 'wa',\n",
       " 'life',\n",
       " 'yes',\n",
       " 'else',\n",
       " 'georgia',\n",
       " 'wish',\n",
       " 'news',\n",
       " 'gotta',\n",
       " 'must',\n",
       " 'xmen',\n",
       " 'crappy',\n",
       " 'god',\n",
       " 'seeing',\n",
       " 'rings',\n",
       " 'interesting',\n",
       " 'aweso',\n",
       " 'end',\n",
       " 'part',\n",
       " 'win',\n",
       " 'agree',\n",
       " 'stayed',\n",
       " 'stinks',\n",
       " 'mother',\n",
       " 'majorly',\n",
       " 'fun',\n",
       " 'donkey',\n",
       " 'something',\n",
       " 'came',\n",
       " 'except',\n",
       " 'btw',\n",
       " 'fat',\n",
       " 'kick',\n",
       " 'erm',\n",
       " 'religious',\n",
       " 'dumb',\n",
       " 'gun',\n",
       " 'thank',\n",
       " 'shes',\n",
       " 'rereading',\n",
       " 'picture',\n",
       " 'sorry',\n",
       " 'shows',\n",
       " 'given',\n",
       " 'day',\n",
       " 'fire',\n",
       " 'death',\n",
       " 'stuff',\n",
       " 'ma',\n",
       " 'wanna',\n",
       " 'case',\n",
       " 'whole',\n",
       " 'board',\n",
       " 'lost',\n",
       " 'year',\n",
       " 'tautou',\n",
       " 'yet',\n",
       " 'hell',\n",
       " 'nearly',\n",
       " 'give',\n",
       " 'brokebac',\n",
       " 'goblet',\n",
       " 'doesnt',\n",
       " 'pot',\n",
       " 'might',\n",
       " 'f',\n",
       " 'gorgeous',\n",
       " 'havent',\n",
       " 'fo',\n",
       " 'critics',\n",
       " 'least',\n",
       " 'n',\n",
       " 'top',\n",
       " 'disliked',\n",
       " 'stupi',\n",
       " 'wondering',\n",
       " 'deal',\n",
       " 'none',\n",
       " 'anyways',\n",
       " 'fandom',\n",
       " 'incredibly',\n",
       " 'rather',\n",
       " 'gift',\n",
       " 'come',\n",
       " 'sexy',\n",
       " 'conclusion',\n",
       " 'sucking',\n",
       " 'besides',\n",
       " 'audrey',\n",
       " 'jake',\n",
       " 'hope',\n",
       " 'k',\n",
       " 'take',\n",
       " 'someone',\n",
       " 'whi',\n",
       " 'films',\n",
       " 'play',\n",
       " 'told',\n",
       " 'christian',\n",
       " 'sounds',\n",
       " 'asian',\n",
       " 'la',\n",
       " 'world',\n",
       " 'days',\n",
       " 'conquering',\n",
       " 'major',\n",
       " 'funny',\n",
       " 'expected',\n",
       " 'icons',\n",
       " 'mountai',\n",
       " 'lousy',\n",
       " 'lame',\n",
       " '6th',\n",
       " 'cowboys',\n",
       " 'piece',\n",
       " 'drive',\n",
       " 'damn',\n",
       " 'girls',\n",
       " 'wont',\n",
       " 'awes',\n",
       " 'nc',\n",
       " 'definitely',\n",
       " 'week',\n",
       " 'knights',\n",
       " 'nt',\n",
       " 'tale',\n",
       " 'makes',\n",
       " 'prin',\n",
       " 'unbelievably',\n",
       " 'home',\n",
       " 'cuz',\n",
       " 'g',\n",
       " 'codes',\n",
       " 'laid',\n",
       " 'tickets',\n",
       " 'cold',\n",
       " 'ask',\n",
       " 'draw',\n",
       " 'tv',\n",
       " 'oscar',\n",
       " 'marvel',\n",
       " 'mi3',\n",
       " 'yesterday',\n",
       " 'pages',\n",
       " 'dance',\n",
       " 'christmas',\n",
       " 'fabulous',\n",
       " 'couple',\n",
       " 'attempt',\n",
       " 'word',\n",
       " 'haunt',\n",
       " 'lee',\n",
       " 'next',\n",
       " 'sivullinen',\n",
       " 'compared',\n",
       " 'kid',\n",
       " 'sooo',\n",
       " 'vampire',\n",
       " 'cringe',\n",
       " 'abou',\n",
       " 'playing',\n",
       " 'different',\n",
       " 'classes',\n",
       " 'dislike',\n",
       " 'level',\n",
       " 'let',\n",
       " 'j',\n",
       " 'guys',\n",
       " 'ha',\n",
       " 'adorable',\n",
       " 'although',\n",
       " 'happy',\n",
       " 'halfblood',\n",
       " 'score',\n",
       " 'doesn',\n",
       " 'girl',\n",
       " 'codesucked',\n",
       " 'cocktail',\n",
       " 'sucke',\n",
       " 'everything',\n",
       " 'idea',\n",
       " 'yea',\n",
       " 'normal',\n",
       " 'exquisite',\n",
       " 'nothing',\n",
       " 'theaters',\n",
       " 'hard',\n",
       " 'phillip',\n",
       " 'funniest',\n",
       " 'record',\n",
       " 'gaither',\n",
       " 'godawful',\n",
       " 'mothers',\n",
       " 'loathe',\n",
       " 'bul',\n",
       " 'hogwarts',\n",
       " 'jesus',\n",
       " 'academy',\n",
       " 'fault',\n",
       " 'ultimate',\n",
       " 'blame',\n",
       " 'many',\n",
       " 'choice',\n",
       " 'thousand',\n",
       " 'dad',\n",
       " 'marcia',\n",
       " 'scar',\n",
       " 'quizzes',\n",
       " 'short',\n",
       " 'club',\n",
       " 'hating',\n",
       " 'house',\n",
       " 'character',\n",
       " 'passion',\n",
       " 'food',\n",
       " 'wesley',\n",
       " 'wasnt',\n",
       " 'sing',\n",
       " 'showing',\n",
       " 'rowling',\n",
       " 'felt',\n",
       " 'kids',\n",
       " 'ang',\n",
       " 'generally',\n",
       " 'party',\n",
       " 'acting',\n",
       " 'need',\n",
       " 'friend',\n",
       " 'imp',\n",
       " 'decided',\n",
       " 'franchise',\n",
       " 'flick',\n",
       " 'fanfiction',\n",
       " 'comes',\n",
       " 'future',\n",
       " 'finally',\n",
       " 'ending',\n",
       " 'lie',\n",
       " 'fireworks',\n",
       " 'friendships',\n",
       " 'heart',\n",
       " 'cry',\n",
       " 'whether',\n",
       " 'opened',\n",
       " 'suppose',\n",
       " 'al',\n",
       " 'yall',\n",
       " 'around',\n",
       " 'l',\n",
       " 'novel',\n",
       " 'mouth',\n",
       " 'watson',\n",
       " 'stone',\n",
       " 'instead',\n",
       " 'emma',\n",
       " 'po',\n",
       " 'super',\n",
       " 'sometimes',\n",
       " 'definately',\n",
       " 'idk',\n",
       " 'mention',\n",
       " 'feel',\n",
       " 'ran',\n",
       " 'woo',\n",
       " 'hero',\n",
       " 'please',\n",
       " 'dragons',\n",
       " 'turn',\n",
       " 'education',\n",
       " 'hoover',\n",
       " 'especially',\n",
       " 'royally',\n",
       " 'lin',\n",
       " 'rant',\n",
       " 'awesomest',\n",
       " 'kind',\n",
       " 'use',\n",
       " 'thinks',\n",
       " 'mrs',\n",
       " 'hollywood',\n",
       " 'knew',\n",
       " 'favourite',\n",
       " 'ban',\n",
       " 'requested',\n",
       " 'vi',\n",
       " 'played',\n",
       " 'work',\n",
       " 'chris',\n",
       " 'ide',\n",
       " 'lets',\n",
       " 'hold',\n",
       " 'weekend',\n",
       " 'comment',\n",
       " 'bad',\n",
       " 'looking',\n",
       " 'impossibl',\n",
       " '33',\n",
       " 'aaron',\n",
       " 'lov',\n",
       " 'lah',\n",
       " '10',\n",
       " 'ya',\n",
       " 'stop',\n",
       " 'dictate',\n",
       " 'point',\n",
       " 'loves',\n",
       " 'write',\n",
       " 'co',\n",
       " 'trailers',\n",
       " 'tho',\n",
       " 'aka',\n",
       " 'goth',\n",
       " 'cried',\n",
       " 'strangely',\n",
       " 'x',\n",
       " '4',\n",
       " 'gla',\n",
       " 'decent',\n",
       " 'u',\n",
       " 'cod',\n",
       " 'forgotten',\n",
       " 'murderballimmediately',\n",
       " 'figures',\n",
       " 'fell',\n",
       " 'whenever',\n",
       " 'john',\n",
       " 'reply',\n",
       " 'v',\n",
       " 'worthless',\n",
       " 'shout',\n",
       " 'brilliant',\n",
       " 'raises',\n",
       " 'omen',\n",
       " 'kickass',\n",
       " 'mocking',\n",
       " 'narnia',\n",
       " 'mormon',\n",
       " 'rosie',\n",
       " 'undercover',\n",
       " 'potte',\n",
       " 'author',\n",
       " 'hoff',\n",
       " 'preview',\n",
       " 'independent',\n",
       " 'equus',\n",
       " 'springers',\n",
       " 'gavin',\n",
       " 'classic',\n",
       " 'mo',\n",
       " 'community',\n",
       " 'ser',\n",
       " 'fyimission',\n",
       " 'condemnation',\n",
       " 'believe',\n",
       " 'selfish',\n",
       " 'shiti',\n",
       " 'ignorant',\n",
       " 'offense',\n",
       " 'drove',\n",
       " 'writer',\n",
       " 'favorite',\n",
       " 'carefully',\n",
       " 'ballz',\n",
       " 'edition',\n",
       " 'outnumbered',\n",
       " 'events',\n",
       " 'nothin',\n",
       " 'stitch',\n",
       " 'guts',\n",
       " 'project',\n",
       " 'codethat',\n",
       " 'mcphee',\n",
       " 'decides',\n",
       " 'listens',\n",
       " 'rode',\n",
       " 'd',\n",
       " 'decide',\n",
       " '200',\n",
       " 'poem',\n",
       " 'anime',\n",
       " 'reason',\n",
       " 'talked',\n",
       " '10pm',\n",
       " 'budget',\n",
       " 'slash',\n",
       " 'mindless',\n",
       " 'infuser',\n",
       " 'seeking',\n",
       " 'spells',\n",
       " 'wrote',\n",
       " 'japenese',\n",
       " 'latin',\n",
       " 'kelsey',\n",
       " 'jay',\n",
       " 'lately',\n",
       " 'student',\n",
       " 'heartbra',\n",
       " 'freakin',\n",
       " 'juicy',\n",
       " 'yip',\n",
       " 'fears',\n",
       " 'bec',\n",
       " 'ge',\n",
       " 'possibly',\n",
       " 'without',\n",
       " 'runner',\n",
       " 'supporting',\n",
       " 'tan',\n",
       " 'tragic',\n",
       " 'rep',\n",
       " 'straight',\n",
       " 'main',\n",
       " 'months',\n",
       " 'field',\n",
       " 'arse',\n",
       " 'apart',\n",
       " 'believabl',\n",
       " 'wiccans',\n",
       " 'everytime',\n",
       " 'crashand',\n",
       " 'archive',\n",
       " 'hands',\n",
       " 'defensive',\n",
       " 'free',\n",
       " 'reader',\n",
       " 'portuguese',\n",
       " 'images',\n",
       " 'lesson',\n",
       " 'catch',\n",
       " 'witha',\n",
       " 'side',\n",
       " 'requiem',\n",
       " 'nice',\n",
       " 'involving',\n",
       " 'ew',\n",
       " 'jelly',\n",
       " 'lamas',\n",
       " '70',\n",
       " 'adversity',\n",
       " 'riding',\n",
       " 'bolsters',\n",
       " 'hugged',\n",
       " 'didn',\n",
       " 'anyhow',\n",
       " 'tc',\n",
       " 'haha',\n",
       " 'natur',\n",
       " 'media',\n",
       " 'gettin',\n",
       " 'positions',\n",
       " 'mall',\n",
       " 'interest',\n",
       " 'decompo',\n",
       " 'demeantor',\n",
       " 'along',\n",
       " 'weekhavent',\n",
       " 'surprisingly',\n",
       " 'hermione',\n",
       " '30',\n",
       " 'en',\n",
       " 'soooo',\n",
       " 'doubt',\n",
       " 'sent',\n",
       " 'prediction',\n",
       " 'beautif',\n",
       " 'conside',\n",
       " 'award',\n",
       " 'gettting',\n",
       " 'background',\n",
       " 'suing',\n",
       " 'feast',\n",
       " 'fridayharry',\n",
       " 'booki',\n",
       " 'taki',\n",
       " 'seriously',\n",
       " 'yuck',\n",
       " 'copy',\n",
       " 'twist',\n",
       " 'facebook',\n",
       " 'asleep',\n",
       " 'anne',\n",
       " 'barnyard',\n",
       " 'lore',\n",
       " 'clips',\n",
       " 'hoffman',\n",
       " 'actio',\n",
       " 'crusade',\n",
       " 'iv',\n",
       " 'pull',\n",
       " 'bookda',\n",
       " 'thirdlythe',\n",
       " 'found',\n",
       " 'scorebrokeback',\n",
       " 'idiot',\n",
       " 'af',\n",
       " 'denial',\n",
       " 'x3good',\n",
       " 'awkward',\n",
       " 'often',\n",
       " 'disappointed',\n",
       " 'absolute',\n",
       " 'gayscientologistmission',\n",
       " 'lines',\n",
       " 'hugh',\n",
       " 'sucky',\n",
       " 'picnic',\n",
       " 'cars',\n",
       " 'lotr',\n",
       " 'quip',\n",
       " 'roommate',\n",
       " 'dumbest',\n",
       " 'surprised',\n",
       " 'involved',\n",
       " 'radio',\n",
       " 'cruises',\n",
       " 'rps',\n",
       " 'controversy',\n",
       " 'bible',\n",
       " 'correct',\n",
       " 'jones',\n",
       " 'matters',\n",
       " 'reaction',\n",
       " 'entire',\n",
       " 'bitch',\n",
       " 'weeeellllllll',\n",
       " 'kite',\n",
       " 'touching',\n",
       " 'due',\n",
       " 'rented',\n",
       " 'superman',\n",
       " 'hahash',\n",
       " 'seems',\n",
       " 'rel',\n",
       " 'decaying',\n",
       " 'itz',\n",
       " 'happiness',\n",
       " 'phoenix',\n",
       " 'figure',\n",
       " 'fits',\n",
       " 'fix',\n",
       " 'experience',\n",
       " '3333',\n",
       " 'overlooking',\n",
       " 'aimee',\n",
       " 'ago',\n",
       " 'match',\n",
       " 'tired',\n",
       " 'rice',\n",
       " 'interview',\n",
       " 'packed',\n",
       " 'task',\n",
       " 'grea',\n",
       " 'credit',\n",
       " 'textsfantasy',\n",
       " 'inten',\n",
       " 'saidharry',\n",
       " 'fanfic',\n",
       " 'section',\n",
       " 'etc',\n",
       " 'honestly',\n",
       " 'brooke',\n",
       " 'working',\n",
       " 'television',\n",
       " 'following',\n",
       " 'eragons',\n",
       " 'headmistresss',\n",
       " 'personaly',\n",
       " 'creature',\n",
       " 'erins',\n",
       " 'simply',\n",
       " 'mountainbeautiful',\n",
       " 'writes',\n",
       " 'ot',\n",
       " 'worst',\n",
       " 'canceled',\n",
       " 'conversations',\n",
       " 'banning',\n",
       " 'unfortunate',\n",
       " 'cussler',\n",
       " 'pink',\n",
       " 'adore',\n",
       " 'dissapointed',\n",
       " 'supper',\n",
       " 'beach',\n",
       " 'heartbreaking',\n",
       " 'tournament',\n",
       " 'tiny',\n",
       " 'howmission',\n",
       " 'men',\n",
       " 'starred',\n",
       " 'portugal',\n",
       " 'homophobic',\n",
       " 'asking',\n",
       " 'freaginlove',\n",
       " 'hedge',\n",
       " 'anatomy',\n",
       " 'meganpenworthy',\n",
       " 'seem',\n",
       " 'money',\n",
       " 'thanks',\n",
       " 'consider',\n",
       " 'frenzied',\n",
       " 'silver',\n",
       " 'eh',\n",
       " 'fiber',\n",
       " 'disney',\n",
       " 'cowan',\n",
       " 'ca',\n",
       " 'state',\n",
       " 'deciding',\n",
       " 'actor',\n",
       " 'pictures',\n",
       " 'tr',\n",
       " 'artemis',\n",
       " 'harr',\n",
       " 'shipmates',\n",
       " 'com',\n",
       " 'durno',\n",
       " 'davinci',\n",
       " 'teaches',\n",
       " 'class',\n",
       " 'rest',\n",
       " 'team',\n",
       " '1brokeback',\n",
       " 'abso',\n",
       " 'titanic',\n",
       " 'tome',\n",
       " 'carsgood',\n",
       " 'washarry',\n",
       " 'characterization',\n",
       " 'empty',\n",
       " 'romantic',\n",
       " 'lilo',\n",
       " 'thriller',\n",
       " 'machine',\n",
       " 'bound',\n",
       " 'related',\n",
       " 'friggin',\n",
       " 'post',\n",
       " 'amazes',\n",
       " 'plus',\n",
       " 'shut',\n",
       " 'wacked',\n",
       " 'pic',\n",
       " 'pla',\n",
       " 'spontaneously',\n",
       " 'look',\n",
       " 'filmsorcerers',\n",
       " 'teac',\n",
       " 'recently',\n",
       " 'silly',\n",
       " 'small',\n",
       " 'screens',\n",
       " 'comprehend',\n",
       " 'check',\n",
       " 'spanish',\n",
       " 'futile',\n",
       " 'invisible',\n",
       " '2nd',\n",
       " 'tells',\n",
       " 'liberal',\n",
       " 'pirated',\n",
       " 'joiners',\n",
       " 'settin',\n",
       " 'refusing',\n",
       " 'joan',\n",
       " 'looked',\n",
       " 'eve',\n",
       " 'deluded',\n",
       " 'jessica',\n",
       " 'bitter',\n",
       " 'meeting',\n",
       " 'bough',\n",
       " 'orig',\n",
       " 'briefly',\n",
       " 'updatehavent',\n",
       " 'haunted',\n",
       " 'hot',\n",
       " 'highly',\n",
       " 'baby',\n",
       " 'quaintly',\n",
       " 'apolo',\n",
       " 'clearly',\n",
       " 'lynne',\n",
       " '1st',\n",
       " 'scientology',\n",
       " 'seemed',\n",
       " 'threegeneration',\n",
       " 'staying',\n",
       " 'muahahaaha',\n",
       " 'fall',\n",
       " 'ony',\n",
       " 'intellectual',\n",
       " 'taking',\n",
       " 'street',\n",
       " 'acoustic',\n",
       " 'devastate',\n",
       " 'othe',\n",
       " 'fi',\n",
       " 'clit',\n",
       " 'gladly',\n",
       " 'emotes',\n",
       " 'started',\n",
       " 'genres',\n",
       " 'invisibility',\n",
       " 'festivities',\n",
       " 'encourage',\n",
       " ...]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vec.fit(refined_df).vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that above LOC sets up a count_vec model to count and then vectorize the data from in the ‘refined_tokens’ column and create a new column named ‘cv_features’ to contain this vector. And applies this transformation model to the refined_df, which contained tokenized documents (after tokenization and stopword removal was performed on the original data). The resulting dataframe, cv_df, now includes a new vector column that shows the count of each word in each document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output from cv_features column ; \n",
    "(1706,[0,1,2,174,180,181,184],[1.0,1.0,1.0,1.0,1.0,1.0,1.0]) :  Indicates the number of words in BoW or Vocabulary, indicates th eindex positions, indicates the number of times word appears in the document respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cv_df = cv_df.select(['cv_features','Label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Feature Engineering, we convert cv_features to vector using vectorassembler and the resulting vector 'cv_features_vec' is used as independant variable in the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols=['cv_features'],outputCol='cv_features_vec')\n",
    "model_cv_df = df_assembler.transform(model_cv_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cv_features: vector (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- cv_features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_cv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training and test datasets using 75/25 split\n",
    "trainingcv_df,testcv_df = model_cv_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using groupby in the training and test datasets, we check the distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 2985|\n",
      "|  0.0| 2343|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainingcv_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0|  924|\n",
      "|  0.0|  738|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testcv_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "log_reg_cv = LogisticRegression(featuresCol = 'cv_features_vec', labelCol = 'Label').fit(trainingcv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using evaluation metrics, we train the model and evaluate how well it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC:0.9999994995606858\n",
      "Weighted Accuracy:0.9998123123123123\n",
      "Weighted Recall:0.9998123123123124\n",
      "Weighted Precision:0.9998123751682023\n",
      "Weighted F1 Measure:0.9998123079982135\n"
     ]
    }
   ],
   "source": [
    "training_summary_cv = log_reg_cv.summary\n",
    "print(\"Area Under ROC:\" + str(training_summary_cv.areaUnderROC))\n",
    "print(\"Weighted Accuracy:\" + str(training_summary_cv.accuracy))\n",
    "print(\"Weighted Recall:\" + str(training_summary_cv.weightedRecall))\n",
    "print(\"Weighted Precision:\" + str(training_summary_cv.weightedPrecision))\n",
    "print(\"Weighted F1 Measure:\" + str(training_summary_cv.weightedFMeasure()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cv = log_reg_cv.evaluate(testcv_df).predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+----------------------------------+----------------------------------------+------------------------------------------+----------+\n",
      "|cv_features                       |Label|cv_features_vec                   |rawPrediction                           |probability                               |prediction|\n",
      "+----------------------------------+-----+----------------------------------+----------------------------------------+------------------------------------------+----------+\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "|(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|1.0  |(1706,[0,1,2,7],[1.0,1.0,1.0,1.0])|[-22.619734281860328,22.619734281860328]|[1.500977628180061E-10,0.9999999998499023]|1.0       |\n",
      "+----------------------------------+-----+----------------------------------+----------------------------------------+------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_cv.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+------------------------------------------+\n",
      "|label|prediction|probability                               |\n",
      "+-----+----------+------------------------------------------+\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "|1.0  |1.0       |[1.500977628180061E-10,0.9999999998499023]|\n",
      "+-----+----------+------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_cv.select('label', 'prediction','probability').show(20,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "true_postives_cv = results_cv[(results_cv.Label == 1) & (results_cv.prediction == 1)].count()\n",
    "true_negatives_cv = results_cv[(results_cv.Label == 0) & (results_cv.prediction == 0)].count()\n",
    "false_positives_cv = results_cv[(results_cv.Label == 0) & (results_cv.prediction == 1)].count()\n",
    "false_negatives_cv = results_cv[(results_cv.Label == 1) & (results_cv.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9902597402597403\n"
     ]
    }
   ],
   "source": [
    "recall_cv = float(true_postives_cv)/(true_postives_cv + false_negatives_cv)\n",
    "print(recall_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "915 715\n",
      "23 9\n"
     ]
    }
   ],
   "source": [
    "print(true_postives_cv, true_negatives_cv)\n",
    "print(false_positives_cv, false_negatives_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9754797441364605\n"
     ]
    }
   ],
   "source": [
    "precision_cv = float(true_postives_cv) / (true_postives_cv + false_positives_cv)\n",
    "print(precision_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9807460890493381\n"
     ]
    }
   ],
   "source": [
    "accuracy_cv = float((true_postives_cv + true_negatives_cv) /(results_cv.count()))\n",
    "print(accuracy_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9828141783029002"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score_cv = 2*((precision_cv*recall_cv)/(precision_cv + recall_cv))\n",
    "F1_score_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision score of 97% says 97% of the time it is predicting the model well. From the above metrics we can say model is predicting really well. But it also could be biased. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF,IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_vec = HashingTF(inputCol='refined_tokens',outputCol='tf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashing_df = hashing_vec.transform(refined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|refined_tokens                                                  |tf_features                                                                                                            |\n",
      "+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|[da, vinci, code, book, awesome]                                |(262144,[82495,93284,111793,189113,235054],[1.0,1.0,1.0,1.0,1.0])                                                      |\n",
      "|[first, clive, cussler, ive, ever, read, even, books, like, rel]|(262144,[47372,53570,82111,120246,139559,174966,182843,203802,208258,227467],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|[liked, da, vinci, code, lot]                                   |(262144,[32675,93284,111793,128231,235054],[1.0,1.0,1.0,1.0,1.0])                                                      |\n",
      "|[liked, da, vinci, code, lot]                                   |(262144,[32675,93284,111793,128231,235054],[1.0,1.0,1.0,1.0,1.0])                                                      |\n",
      "+----------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashing_df.select(['refined_tokens','tf_features']).show(4,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- review_nopunct: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- refined_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_count: integer (nullable = false)\n",
      " |-- tf_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashing_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = IDF(inputCol = 'tf_features', outputCol = 'tf_idf_features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_df = tf_idf_vec.fit(hashing_df).transform(hashing_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|tf_idf_features                                                                                                                                                                                                                                                    |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|(262144,[82495,93284,111793,189113,235054],[1.928750258373558,1.269640397597574,1.2610218398134343,5.045716396741666,1.2620319409094194])                                                                                                                          |\n",
      "|(262144,[47372,53570,82111,120246,139559,174966,182843,203802,208258,227467],[5.856646612957994,4.227406073227714,4.237258369670726,8.15923170595204,8.15923170595204,5.716884670582836,6.5497937935179396,6.5497937935179396,2.6996461918078807,8.15923170595204])|\n",
      "|(262144,[32675,93284,111793,128231,235054],[4.267411407841413,1.269640397597574,1.2610218398134343,6.019165542455769,1.2620319409094194])                                                                                                                          |\n",
      "|(262144,[32675,93284,111793,128231,235054],[4.267411407841413,1.269640397597574,1.2610218398134343,6.019165542455769,1.2620319409094194])                                                                                                                          |\n",
      "|(262144,[5765,32675,93284,111793,188304,193996,235054,237388],[7.7537665978438755,4.267411407841413,1.269640397597574,1.2610218398134343,6.5497937935179396,8.15923170595204,1.2620319409094194,8.15923170595204])                                                 |\n",
      "|(262144,[28658,105591,146139,174966,220659,243418,249180],[4.386470767857402,8.15923170595204,4.151898520719569,5.716884670582836,8.15923170595204,8.15923170595204,2.4242736138273893])                                                                           |\n",
      "|(262144,[33933,93284,111793,115917,173297,190256,224769,235054],[3.3229497990005616,1.269640397597574,1.2610218398134343,4.409727630021669,7.7537665978438755,3.6933235872974564,7.7537665978438755,1.2620319409094194])                                           |\n",
      "|(262144,[37254,93284,103648,111793,138356,235054,242361],[8.15923170595204,1.269640397597574,8.15923170595204,1.2610218398134343,4.341519379995135,1.2620319409094194,5.716884670582836])                                                                          |\n",
      "|(262144,[93284,111793,113432,132975,155321,235054],[1.269640397597574,1.2610218398134343,5.485083056525511,6.454483613713615,2.3480907129753392,1.2620319409094194])                                                                                               |\n",
      "|(262144,[93284,111793,113432,175449,189113,235054,242361],[1.269640397597574,1.2610218398134343,5.485083056525511,5.4511815048498296,5.045716396741666,1.2620319409094194,5.716884670582836])                                                                      |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_df.select(['tf_idf_features']).show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Review: string (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- length: integer (nullable = true)\n",
      " |-- review_nopunct: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- refined_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- token_count: integer (nullable = false)\n",
      " |-- tf_features: vector (nullable = true)\n",
      " |-- tf_idf_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text_df = tf_idf_df.select(['tf_idf_features','Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assembler = VectorAssembler(inputCols = ['tf_idf_features'],outputCol = 'tf_idf_features_vec')\n",
    "model_text_df = df_assembler.transform(model_text_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- tf_idf_features: vector (nullable = true)\n",
      " |-- Label: float (nullable = true)\n",
      " |-- tf_idf_features_vec: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_text_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_text_df = model_text_df.select('Label', 'tf_idf_features_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Data for building LogisticRegression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data \n",
    "training_df,test_df = model_text_df.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 2907|\n",
      "|  0.0| 2292|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|Label|count|\n",
      "+-----+-----+\n",
      "|  1.0| 1002|\n",
      "|  0.0|  789|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_df.groupBy('Label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol = 'tf_idf_features_vec', labelCol = 'Label').fit(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running into P4 java error at this time.. I have the results of training summary in the last cell. \n",
    "#Please check that or re-run the below cell \n",
    "#this is not any other error, it can't reach java sometimes and sometimes it works on my system. \n",
    "#Kindly deal with the below cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC:0.9999984991394065\n",
      "Weighted Accuracy:0.9998076553183305\n",
      "Weighted Recall:0.9998076553183304\n",
      "Weighted Precision:0.9998077392017504\n",
      "Weighted F1 Measure:0.9998076597494032\n"
     ]
    }
   ],
   "source": [
    "training_summary = log_reg.summary\n",
    "print(\"Area Under ROC:\" + str(training_summary.areaUnderROC))\n",
    "print(\"Weighted Accuracy:\" + str(training_summary.accuracy))\n",
    "print(\"Weighted Recall:\" + str(training_summary.weightedRecall))\n",
    "print(\"Weighted Precision:\" + str(training_summary.weightedPrecision))\n",
    "print(\"Weighted F1 Measure:\" + str(training_summary.weightedFMeasure()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AUC should ideally be near to 1 which is good.\n",
    "All others being 1 indicated we have balanced data.. also because of tokenizer, its an overfitted model. It could be biased coz its over fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = log_reg.evaluate(test_df).predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives prob and prediction values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------------------+----------+\n",
      "|tf_idf_features                                                                                                                                              |Label|tf_idf_features_vec                                                                                                                                          |rawPrediction                           |probability                                |prediction|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------------------+----------+\n",
      "|(262144,[14,535,17291,71225,186480,197995],[3.203404648350779,1.3573926525614521,4.319779393358729,8.15923170595204,1.5785925686670907,1.3529555959197566])  |1.0  |(262144,[14,535,17291,71225,186480,197995],[3.203404648350779,1.3573926525614521,4.319779393358729,8.15923170595204,1.5785925686670907,1.3529555959197566])  |[-22.3486076437676,22.3486076437676]    |[1.9684438091316214E-10,0.9999999998031557]|1.0       |\n",
      "|(262144,[14,535,33933,174966,197339,197995],[3.203404648350779,1.3573926525614521,3.3229497990005616,5.716884670582836,5.556542020507656,1.3529555959197566])|1.0  |(262144,[14,535,33933,174966,197339,197995],[3.203404648350779,1.3573926525614521,3.3229497990005616,5.716884670582836,5.556542020507656,1.3529555959197566])|[-18.76398637623106,18.76398637623106]  |[7.094213513244375E-9,0.9999999929057863]  |1.0       |\n",
      "|(262144,[14,535,113432,197995,209304,251329],[3.203404648350779,2.7147853051229043,5.485083056525511,1.3529555959197566,8.15923170595204,3.5948835144842035])|0.0  |(262144,[14,535,113432,197995,209304,251329],[3.203404648350779,2.7147853051229043,5.485083056525511,1.3529555959197566,8.15923170595204,3.5948835144842035])|[-4.74131108862988,4.74131108862988]    |[0.008651691540607888,0.991348308459392]   |1.0       |\n",
      "|(262144,[14,535,140351,155321,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,2.3480907129753392,1.3529555959197566])                       |0.0  |(262144,[14,535,140351,155321,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,2.3480907129753392,1.3529555959197566])                       |[19.325770067793297,-19.325770067793297]|[0.9999999959549424,4.045057498455958E-9]  |0.0       |\n",
      "|(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |0.0  |(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |[17.948109857762017,-17.948109857762017]|[0.9999999839588714,1.6041128625411527E-8] |0.0       |\n",
      "|(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |0.0  |(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |[17.948109857762017,-17.948109857762017]|[0.9999999839588714,1.6041128625411527E-8] |0.0       |\n",
      "|(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |0.0  |(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |[17.948109857762017,-17.948109857762017]|[0.9999999839588714,1.6041128625411527E-8] |0.0       |\n",
      "|(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |0.0  |(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |[17.948109857762017,-17.948109857762017]|[0.9999999839588714,1.6041128625411527E-8] |0.0       |\n",
      "|(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |0.0  |(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |[17.948109857762017,-17.948109857762017]|[0.9999999839588714,1.6041128625411527E-8] |0.0       |\n",
      "|(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |0.0  |(262144,[14,535,140351,197995],[3.203404648350779,1.3573926525614521,3.6374431289029996,1.3529555959197566])                                                 |[17.948109857762017,-17.948109857762017]|[0.9999999839588714,1.6041128625411527E-8] |0.0       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------+-------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------------------------------------------+\n",
      "|label|prediction|probability                                |\n",
      "+-----+----------+-------------------------------------------+\n",
      "|1.0  |1.0       |[1.9684438091316214E-10,0.9999999998031557]|\n",
      "|1.0  |1.0       |[7.094213513244375E-9,0.9999999929057863]  |\n",
      "|0.0  |1.0       |[0.008651691540607888,0.991348308459392]   |\n",
      "|0.0  |0.0       |[0.9999999959549424,4.045057498455958E-9]  |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "|0.0  |0.0       |[0.9999999839588714,1.6041128625411527E-8] |\n",
      "+-----+----------+-------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('label', 'prediction','probability').show(20,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "label and prediction being same values implies they are predicting the same\n",
    "\n",
    "This implies train data and test data are predicting the same and also overfitting\n",
    "\n",
    "True measure is evaluation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "true_postives = results[(results.Label == 1) & (results.prediction == 1)].count()\n",
    "true_negatives = results[(results.Label == 0) & (results.prediction == 0)].count()\n",
    "false_positives = results[(results.Label == 0) & (results.prediction == 1)].count()\n",
    "false_negatives = results[(results.Label == 1) & (results.prediction == 0)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9880239520958084\n"
     ]
    }
   ],
   "source": [
    "recall = float(true_postives)/(true_postives + false_negatives)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "990 766\n",
      "23 12\n"
     ]
    }
   ],
   "source": [
    "print(true_postives, true_negatives)\n",
    "print(false_positives, false_negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majority of them are tp and tn\n",
    "remainig are fp and fn\n",
    "\n",
    "Which implies reviews are being well differentiated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9772951628825272\n"
     ]
    }
   ],
   "source": [
    "precision = float(true_postives) / (true_postives + false_positives)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9804578447794529\n"
     ]
    }
   ],
   "source": [
    "accuracy=float((true_postives+true_negatives) /(results.count()))\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9826302729528535"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1_score = 2*((precision*recall)/(precision + recall))\n",
    "F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High precision and High accuracy implies the model is trained well and hence predicting well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer\n",
      "recall_cv 99.02597402597402\n",
      "precision_cv 97.54797441364606\n",
      "true_postives_cv, true_negatives_cv:  915 715\n",
      "false_positives_cv, false_negatives_cv:  23 9\n",
      "accuracy_cv 98.07460890493381\n",
      "********************************************************************************\n",
      "Area Under ROC:0.9999994995606858\n",
      "Weighted Accuracy:0.9998123123123123\n",
      "Weighted Recall:0.9998123123123124\n",
      "Weighted Precision:0.9998123751682023\n",
      "Weighted F1 Measure:0.9998123079982135\n",
      "************************************************************************************************************************\n",
      "TF-IDF\n",
      "recall 98.80239520958084\n",
      "true_postives, true_negatives:  990 766\n",
      "false_positives, false_negatives:  23 12\n",
      "precision 97.72951628825271\n",
      "accuracy 98.04578447794529\n",
      "********************************************************************************\n",
      "Area Under ROC:0.9999984991394065\n",
      "Weighted Accuracy:0.9998076553183305\n",
      "Weighted Recall:0.9998076553183304\n",
      "Weighted Precision:0.9998077392017504\n",
      "Weighted F1 Measure:0.9998076597494032\n"
     ]
    }
   ],
   "source": [
    "print(\"CountVectorizer\")\n",
    "print(\"recall_cv\", recall_cv*100)\n",
    "print(\"precision_cv\", precision_cv*100)\n",
    "print(\"true_postives_cv, true_negatives_cv: \", true_postives_cv, true_negatives_cv)\n",
    "print(\"false_positives_cv, false_negatives_cv: \", false_positives_cv, false_negatives_cv)\n",
    "print(\"accuracy_cv\", accuracy_cv*100)\n",
    "print('****'*20)\n",
    "print(\"Area Under ROC:\" + str(training_summary_cv.areaUnderROC))\n",
    "print(\"Weighted Accuracy:\" + str(training_summary_cv.accuracy))\n",
    "print(\"Weighted Recall:\" + str(training_summary_cv.weightedRecall))\n",
    "print(\"Weighted Precision:\" + str(training_summary_cv.weightedPrecision))\n",
    "print(\"Weighted F1 Measure:\" + str(training_summary_cv.weightedFMeasure()))\n",
    "print('****'*30)\n",
    "print(\"TF-IDF\")\n",
    "print(\"recall\", recall*100)\n",
    "print(\"true_postives, true_negatives: \", true_postives, true_negatives)\n",
    "print(\"false_positives, false_negatives: \", false_positives, false_negatives)\n",
    "print(\"precision\", precision*100)\n",
    "print(\"accuracy\", accuracy*100)\n",
    "print('****'*20)\n",
    "print(\"Area Under ROC:\" + str(training_summary.areaUnderROC))\n",
    "print(\"Weighted Accuracy:\" + str(training_summary.accuracy))\n",
    "print(\"Weighted Recall:\" + str(training_summary.weightedRecall))\n",
    "print(\"Weighted Precision:\" + str(training_summary.weightedPrecision))\n",
    "print(\"Weighted F1 Measure:\" + str(training_summary.weightedFMeasure()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, By using TF-IDF, evaluation metrics have better results than the results from count vectorisation.\n",
    "\n",
    "From the results we can see that the metrics are almost similar either with tf-idf or countvectoriser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
